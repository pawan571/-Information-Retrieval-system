{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSL_zfsVmaEc"
      },
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re # Import regular expressions library\n",
        "from collections import defaultdict , Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Download the 'stopwords' resource\n",
        "nltk.download('stopwords')\n",
        "# Initialize the stop words and lemmatizer\n",
        "STOPWORDS = set(stopwords.words ('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC4mdei6zXg5",
        "outputId": "6fd4d53c-5e01-49f3-d3b8-7ff53f047df4"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Text Files"
      ],
      "metadata": {
        "id": "JHZmFhHa2Shz"
      }
    },
    {
      "source": [
        "def load_text_files(folder_path):\n",
        "    \"\"\"Reads all files in a folder and returns a dictionary\n",
        "    with filenames as keys and content as values.\"\"\"\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "                doc_id_to_filename[doc_id] = filename  # Map doc_id to filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "            doc_id += 1  # Increment document ID for the next file\n",
        "\n",
        "    return data, doc_id_to_filename\n",
        "\n",
        "# Example folder path: Replace this with your folder path\n",
        "folder_path = \"/content/drive/MyDrive/Assignment 2: IR\"\n",
        "data, doc_id_to_filename = load_text_files(folder_path)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LpgNd4ep14Zo"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Dataset and Tokenize"
      ],
      "metadata": {
        "id": "3RwjkobPv8d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Performs text cleaning: removing special characters, digits, tokenization, stopword removal, and lemmatization.\"\"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and punctuation using regular expressions\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Keeps only alphanumeric characters and spaces\n",
        "\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    #remove digits from dataset\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Test cleaning function with a sample text\n",
        "print(clean_text(data[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHyt0fEGy5TP",
        "outputId": "be988f1f-17dd-4dd4-fbc1-e884adb4967a"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ever', 'since', 'installed', 'latest', 'patch', 'phone', 'overheating', 'like', 'crazy', 'look', 'user', 'report', 'overheating', 'issue', 'installing', 'patch', 'requesting', 'assistance', 'loyalty', 'point', 'credited', 'instantly', 'goal', 'met', 'keep', 'eye', 'confirmation', 'email', 'almost', 'rt', 'support', 'confirms', 'loyalty', 'point', 'available', 'immediately', 'email', 'sent', 'upon', 'reaching', 'goal', 'flyhighairlines', 'thanks', 'info', 'super', 'helpful', 'user', 'thanks', 'flyhighairlines', 'provided', 'assistance', 'indicating', 'response', 'helped', 'query', 'flyhighairlines', 'wondering', 'able', 'upgrade', 'seat', 'moment', 'earn', 'enough', 'point', 'waiting', 'period', 'user', 'seek', 'clarification', 'flyhighairlines', 'regarding', 'upgrade', 'seat', 'reaching', 'required', 'point', 'help', 'please', 'dm', 'u', 'information', 'resolve', 'issue', 'customer', 'support', 'offer', 'help', 'via', 'dm', 'asking', 'company', 'discount', 'information', 'better', 'assist', 'user']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverted Index Construction"
      ],
      "metadata": {
        "id": "_tbhOIITx0Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(data):\n",
        "    \"\"\"Builds an inverted index from the cleaned text data and tracks term frequencies.\"\"\"\n",
        "    inverted_index = defaultdict(set)\n",
        "    term_frequencies = Counter()  # Track term frequencies\n",
        "    for doc_id, content in data.items():\n",
        "        cleaned_tokens = clean_text(content)\n",
        "        for token in cleaned_tokens:\n",
        "            inverted_index[token].add(doc_id)\n",
        "            term_frequencies[token] += 1\n",
        "    return inverted_index, term_frequencies"
      ],
      "metadata": {
        "id": "5SOuNyKCyAeF"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boolean Query Processing: AND, OR, and NOT Operations:"
      ],
      "metadata": {
        "id": "8TtXIKQ8yI7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_query(query, inverted_index, doc_id_to_filename):\n",
        "    \"\"\"Processes a Boolean query and returns the matching documents.\"\"\"\n",
        "    tokens = query.lower().split()\n",
        "    result_set = set(doc_id_to_filename.keys())  # Start with all documents in the dataset\n",
        "\n",
        "    if 'and' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        result_set = inverted_index.get(terms[0], set())  # Start with the first term\n",
        "        for term in terms[1:]:\n",
        "            if term in inverted_index:\n",
        "                result_set = result_set.intersection(inverted_index[term])\n",
        "            else:\n",
        "                result_set = set()  # If a term is missing, no results should be returned\n",
        "\n",
        "    elif 'or' in tokens:\n",
        "        result_set = set()  # OR should start with an empty set\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        for term in terms:\n",
        "            if term in inverted_index:\n",
        "                result_set = result_set.union(inverted_index[term])\n",
        "\n",
        "    elif 'not' in tokens:\n",
        "        term = tokens[1]  # Get the term after 'NOT'\n",
        "        if term in inverted_index:\n",
        "            result_set = result_set - inverted_index[term]  # Subtract the documents that contain the term\n",
        "        else:\n",
        "            pass  # If the term doesn't exist, do nothing as we subtract nothing\n",
        "\n",
        "    else:\n",
        "        # If no 'AND', 'OR', 'NOT' operators, check if single query term exists\n",
        "        if query in inverted_index:\n",
        "            result_set = inverted_index[query]\n",
        "        else:\n",
        "            result_set = set()  # If query term doesn't exist, return empty set\n",
        "\n",
        "    # Convert doc_ids to filenames\n",
        "    result_filenames = [doc_id_to_filename[doc_id] for doc_id in result_set if doc_id in doc_id_to_filename]\n",
        "\n",
        "    logging.info(f\"Query '{query}' resulted in: {result_filenames}\")\n",
        "\n",
        "    return result_filenames"
      ],
      "metadata": {
        "id": "YZzHOlfmK38z"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load dataset\n",
        "    folder_path = \"/content/drive/MyDrive/Assignment 2: IR\"\n",
        "    data, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    # Build the inverted index and term frequencies\n",
        "    inverted_index, term_frequencies = build_inverted_index(data)\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"Company AND Customer\",\n",
        "        \"Phone OR Information\",\n",
        "        \"NOT Discount\",\n",
        "    ]\n",
        "\n",
        "    # Open a file to write the results\n",
        "    with open(\"Pawan_results.txt\", \"w\") as result_file:\n",
        "        for query in queries:\n",
        "            result = boolean_query(query, inverted_index, doc_id_to_filename)\n",
        "            result_str = f\"Results for '{query}': {result}\\n\"\n",
        "            print(result_str)  # Print to console\n",
        "            result_file.write(result_str)  # Write to file\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "CgAENvGp9w_L",
        "outputId": "948a521f-5c68-4221-afdf-d375385135f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for 'Company AND Customer': ['doc.4.txt', 'doc.6.txt', 'doc.3.txt', 'doc.1.txt']\n",
            "\n",
            "Results for 'Phone OR Information': ['doc.4.txt', 'doc.1.txt', 'doc.2.txt']\n",
            "\n",
            "Results for 'NOT Discount': ['doc.5.txt', 'doc.1.txt', 'doc.7.txt', 'doc.2.txt']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}